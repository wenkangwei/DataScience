{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 4300/6300-001 Applied Data Science (Fall 2020)\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC4300/6300-001 Problem Set #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C. Prepare Data for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Set up environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6bcd323629feac54709554f0d1681964",
     "grade": false,
     "grade_id": "cell-c6ba0cee61e5ae5b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b0d412a858310512b16a93592f32553",
     "grade": false,
     "grade_id": "cell-16750131fd1192ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "housing = pd.read_csv(\"input/housing.csv\")\n",
    "attribute_caps = housing[['median_income', 'housing_median_age', 'median_house_value']].max()\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84e9f31c4fdc4ded3253737437f9a0bd",
     "grade": false,
     "grade_id": "cell-030385611b26855a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Train-Test Split Using Stratified Shuffle Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a9f48c51b93ddaa52d0d48756c59d7d8",
     "grade": false,
     "grade_id": "cell-c6a9d423bffc2e6c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Normally, you can use the __train_test_split()__ method in the __sklearn.model_selection__ model to split the a data set data. \n",
    "\n",
    "However, a simple random split approach sometime may be not ideal. As shown in the histograms below, most of the samples are concentrated in the regions where median_income is in the range of [1, 8]. When we draw training data overwhelmingly from a concentrated region, the trained machine learning model may not work well with other regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc86b0e2b4c5c86fba31c9166e9d9c08",
     "grade": false,
     "grade_id": "cell-d3954738e6a4ed40",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(housing, test_size=0.33, random_state=6300)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "for idx, col in enumerate(['median_income', 'median_house_value']):\n",
    "    for df in [housing, train_data, test_data]:\n",
    "        _ = sns.distplot(df[col], bins=20, ax=axes[idx])\n",
    "    _ = axes[idx].set_xlim(housing[col].min(), housing[col].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "74b30b4ad7a4cc26d07cb4d4f5eeb4a4",
     "grade": false,
     "grade_id": "cell-43e873da47b137ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ideally, you may want each stratum of the median income has an adequate representation in the training and test datasets. The stratified sampling method serves this purpose. \n",
    "\n",
    "In scikit_learn, the __sklearn.model_selection.StratifiedShuffleSplit__ create stratified splits (see https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html).\n",
    "\n",
    "__Complete the following code to create a stratified split of training and testing data according to the median_income and compute the means and percents of each bin as shown in the above code.__\n",
    "\n",
    "Hint: \n",
    "+ You can set n_splits=1 in the __StratifiedShuffleSplit__.\n",
    "+ __StratifiedShuffleSplit.split(X, y)__ generate indices to split data into training and test set. The stratification is based on the __y__ label. For this question, what would you pass as the __y__ argument?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "70747a70b0bf00197e9eb1e3869ab35d",
     "grade": true,
     "grade_id": "cell-5c3336a4d951eece",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Cut data by median_income\n",
    "bins = [0., 1.5, 3.0, 4.5, 6.0, 8.0, 10.0, np.inf]\n",
    "housing['income_level'] = pd.cut(housing['median_income'], bins, labels=[1, 2, 3, 4, 5, 6, 7])\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "823f6df9b966b591cf3c8bfc9564b528",
     "grade": false,
     "grade_id": "cell-258f9725d993f312",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_income_levels = pd.DataFrame()\n",
    "# Get median_income mean and count at each income level\n",
    "for df in [housing, train_data, test_data]:\n",
    "    df1 = df.groupby('income_level').agg({'median_income': ['mean', 'count']})\n",
    "    df_income_levels = pd.concat([df_income_levels, df1], axis=1)\n",
    "\n",
    "# Print median_income mean and percentage of samples at each income level\n",
    "df_income_levels.columns = pd.MultiIndex.from_product([['all_data', \"train_data\", \"test_data\"], [\"mean\", \"count\"]])\n",
    "income_level_counts = df_income_levels[('all_data', 'count')]\n",
    "for dataset in ['all_data', 'train_data', 'test_data']:\n",
    "    df_income_levels[(dataset, 'count')] = df_income_levels[(dataset, 'count')]/income_level_counts\n",
    "df_income_levels.columns = pd.MultiIndex.from_product([['all_data', \"train_data\", \"test_data\"], [\"mean\", \"percent\"]])\n",
    "df_income_levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f4e9d4d0680d2b4202101376d0118a7",
     "grade": false,
     "grade_id": "cell-1f619ff8c3f8eee8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 3. Handle Missing Values\n",
    "\n",
    "You may have noticed that the `total_bedrooms` contains some missing data. There are several options to handle attributes with missing values:\n",
    "\n",
    "1. Remove the rows that contain missing features.\n",
    "2. Remove the columns that contain missing features.\n",
    "3. Replace the missing values with new values such as the mean, the median, etc.\n",
    "\n",
    "The DataFrame methods `dropna()`, `drop()`, and `fillna()` can be used to handle missing data in a DataFrame.\n",
    "\n",
    "Assume you choose the replacement option, you may consider a systematic solution that handle the following two scenarios:\n",
    "\n",
    "+ Both the train data and test data need to handle the missing values.\n",
    "+ When your system is going online, attributes that currently do not contain missing values may have missing values.\n",
    "\n",
    "The `SimpleImputer` class in the scikit-leran library (https://scikit-learn.org/stable/modules/impute.html) can accomplish this goal. \n",
    "\n",
    "__Complete the following code to replace all missing values in each column with the means of that column in the train_data and save the cleaned data into a DataFrame named `df_num`.__\n",
    "\n",
    "Hint: Because `SimpleImputer` is applicable to numerical attributes only, the DataFrame you will appply a `SimpleImputer` transform must not contain any non-numerical attribute. You can create a new DataFrame that contains all the numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae734a9f269b86166a9c601985f4c167",
     "grade": false,
     "grade_id": "cell-0f854efeaccddad2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# get all numeric columns\n",
    "df_num = train_data[train_data.dtypes[train_data.dtypes == 'float64'].index]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16471357dc89f8c817195852be230de9",
     "grade": true,
     "grade_id": "cell-67594c95cb69e87f",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test if there are still missing values\n",
    "all(df_num.isna().sum() == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62a5970bc003e95acbd3001b582203a1",
     "grade": true,
     "grade_id": "cell-f769a48828e3368e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test if the number of samples has been changed\n",
    "train_data.shape[0] == df_num.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c94b6392af1bcc98a6476a3d9141e3a",
     "grade": true,
     "grade_id": "cell-c010571d17876b0f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test if the means of the train_data and df_num are same\n",
    "assert np.abs(train_data['total_bedrooms'].mean() - df_num['total_bedrooms'].mean()) <= 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Add Combined Attributes\n",
    "\n",
    "When you build a machine learning model, you may often find that a derived attribute from combing two raw attributes can have a stronger relationship with the target variable. \n",
    "\n",
    "In the housing data, there are four aggregated counts attributes: `total_rooms`, `total_bedrooms`, `population`, and `household`. Intuitively, `median_house_value` will have a stronger relation with  a ratio like `rooms_per_household` than with an aggregation like `total_room`. \n",
    "\n",
    "You can write a program  to test which combination may have a strong correlation as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9035ffae62236b1e21616db677083ea0",
     "grade": false,
     "grade_id": "cell-68914b79ff556470",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "attrs = ['total_rooms', 'total_bedrooms', 'population', 'households', 'median_house_value']\n",
    "df_test = df_num[attrs].copy()\n",
    "derived_attrs = []\n",
    "for attr1 in attrs[:-1]:\n",
    "    for attr2 in attrs[:-1]:\n",
    "        if attr2 == attr1:\n",
    "            continue\n",
    "        derived_attr = attr1+'_'+attr2\n",
    "        derived_attrs.append(derived_attr)\n",
    "        df_test[derived_attr] = df_test[attr1]/df_test[attr2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56739997ff38c620f04753a0d21f863e",
     "grade": false,
     "grade_id": "cell-d573b2483281dcdb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "candidate_attrs = attrs + derived_attrs\n",
    "corrs = df_test.corr()['median_house_value'].drop(index='median_house_value').sort_values(ascending=False)\n",
    "print(corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = corrs.plot.barh(title='Correlation Coefficient b/t attribute with median_house_value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions based on the above results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 4(a)__. Among `households`, `population`, `household_size`, and `household_population_ratio`, which one has the strongest relation with `'median_house_value`? (Note: `household_size = population/households`) Assign your answer to a string variable `answer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0561bad410d8abcd8e66188a3e36d571",
     "grade": true,
     "grade_id": "cell-ccc95c789085d334",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "answer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 4(b)__. Does a higher `room_bedroom_ratio` increase the `median_house_value` or decrease the `median_house_value`? Assign your answer (either `increase` or `descrease`) to a string variable `answer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a82dfa074a584c04df26c03b5e1e24cf",
     "grade": true,
     "grade_id": "cell-a5ab9598906b5db9",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "answer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 4(c)__. Assume you decide to add the three derive attributes, `rooms_per_bedroom`,  `household_per_population`, and `rooms_per_household` to `df_num`, write some code to achieve that goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "46399c0a7217edb07c3b5b27f8ab75f5",
     "grade": false,
     "grade_id": "cell-a727f825ff07d73f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "df_num.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5932db509acec813a9548e5c22a07db3",
     "grade": true,
     "grade_id": "cell-049257c7b9444a2c",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert all([col in df_num for col in ['rooms_per_bedroom', 'household_per_population', 'rooms_per_household']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "00dc6b54319ace149102dc4207864626",
     "grade": true,
     "grade_id": "cell-cd5dd0c9b78b8c05",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.abs(df_num['household_per_population'].mean() - (df_num['households']/df_num['population']).mean()) < 1.0e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Handling Text and Categorical Attributes\n",
    "\n",
    "Categorical data is one type of structured data we frequently see in data science. For example, the position of a football player, the weather of a day, the size of a shirt, the genres of music and country name are all categorical data.\n",
    "\n",
    "In categorical data, the values of a data attribute are discrete and belong to a finite set of groups, which are often known as classes or labels. The values of a categorical data can be numeric or textual.\n",
    "\n",
    "There are two major classes of categorical data: nominal and ordinal. \n",
    "\n",
    "+ For nominal categorical attribute, there is no concept of ordering among the values of the attribute. For example, The types of weathers are nominal data.\n",
    "\n",
    "+ For ordinal categorical attribute, you can place the attributes into some kind of order or scale. For example, you can rate the happiness on a scale of 1-10. Similarly, shoe sizes, education level, income levels, and hurricane categories are examples of ordinal data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 5(a)__. Is __ocean_proximity__ `nominal categorical data` or `ordinal categorical data`? Assign your answer to a string variable `answer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64350b790bf7c92999041fbcf4379822",
     "grade": false,
     "grade_id": "cell-ff04fda7a86ef78a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af835d0b1f22c85f76867b80246488e7",
     "grade": true,
     "grade_id": "cell-7206fa2b5ad1636d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert any(['ordinal' in answer.lower(), 'nominal' in answer.lower()])\n",
    "# There is a hidden test here\n",
    "assert any(['nominal' in answer.lower()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 5(b)__. There are multiple ways to encode categorical data such as using `pandas.get_dummies()` method or `sklearn.preprocessing.OneHotEncoder` (see https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html).\n",
    "\n",
    "__Complete the code that convert the __ocean_proximity__ categorical data into one-hot vectors using `sklearn.preprocessing.OneHotEncoder`. Save the encoded results to a variable named __housing_cat_1hot__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "267be4e7a70a3f40680698dae7a84838",
     "grade": false,
     "grade_id": "cell-2151f2d2eb858dac",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "housing_cat = train_data[[\"ocean_proximity\"]]\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(housing_cat_1hot[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "184b5a1359d9fcc2876349534bce5a4d",
     "grade": true,
     "grade_id": "cell-69bb0cf8df1334fb",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "assert isinstance(housing_cat_1hot, scipy.sparse.csr.csr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ea6b3d7bd020daf7a87373e2ccb1d0b",
     "grade": true,
     "grade_id": "cell-2116da6d35c1bffb",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert housing_cat_1hot.shape[1] == len(train_data[\"ocean_proximity\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Remove Capped Data\n",
    "\n",
    "During exploratory data analysis, we have found that the maximum values of several attributes are capped. It might be a good idea to drop those samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = train_data[['median_income', 'housing_median_age', 'median_house_value']].hist(bins=50, layout=(1, 3), figsize=(18, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 6(a)__. __Write some code to drop the samples whose attribute values are capped.__\n",
    "\n",
    "Hint: You can use the `DataFrame['attr'].max()` method to find the upper cap of the attribute 'attr'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "881f3f949d9150b8e0559ddcf317407d",
     "grade": false,
     "grade_id": "cell-0a1129d6c79acc73",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7cc0967841a36ec2047997464e676805",
     "grade": true,
     "grade_id": "cell-ba8cf0a1780a6d04",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert all([not any(train_data[col] >= attribute_caps[col])\n",
    "            for col in ['median_house_value', 'housing_median_age', 'median_income']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3fcd8f84d3b9bac267a039c8991298e6",
     "grade": false,
     "grade_id": "cell-196b1734afb5c8e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now replot the histogram to validate that those samples are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea88cdb9f411cb087270c46bebfeeafd",
     "grade": false,
     "grade_id": "cell-78243cf865d20089",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "_ = train_data[['median_income', 'housing_median_age', 'median_house_value']].hist(bins=50, layout=(1, 3), figsize=(18, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ef6b5bcf6ed9765274a0f22bdfd6212",
     "grade": false,
     "grade_id": "cell-d290bdf800c127ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 7. Feature Scaling\n",
    "\n",
    "With few exceptions, machine learning algorithms donâ€™t perform well when the input numerical attributes have different scales. For the housing data, the total number of rooms ranges from 6 to 39,320, while the median income ranges from 0 to 15.\n",
    "\n",
    "There are two common approaches to scale the attributes: min-max scaling and standardization.\n",
    "\n",
    "Min-max scaling is quite simple: values are shifted and rescaled so that they end up ranging from 0 to 1.\n",
    "\n",
    "Standardization is different: first, it subtracts the mean value (so standardized values always have a zero mean); then it divides the difference by the standard deviation so that the resulting distribution has unit variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3843c1a77a031d87b10c2b2ac9777d07",
     "grade": false,
     "grade_id": "cell-1bcdd13f8e02dda4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "__Question 7(a)__. __Write some code to standardize the housing train data using `sklearn.preprocessing.StandardScaler`__. Save the transformed data into a variable `num_tr`__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "677130a69b7c2fe0a996a9696f25fd0a",
     "grade": false,
     "grade_id": "cell-cbdf1ef5d26d431f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "df_num = train_data.drop(\"ocean_proximity\", axis=1)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df26dcc94ce0d7ee9d99749d48145df1",
     "grade": false,
     "grade_id": "cell-4153b203ffeee521",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(num_tr)\n",
    "df.columns = df_num.columns\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1ecc210e14b048be07ea603b8e17fc2",
     "grade": true,
     "grade_id": "cell-3e14341d6c59fc21",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(num_tr, np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "35591e60d95d90b62ef57e0cd6eff1fa",
     "grade": true,
     "grade_id": "cell-a69a002eb97eb5cd",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test mean is 0\n",
    "assert all(np.abs(df.mean()) < 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d3ed061c3124a720720bd39010aae12",
     "grade": true,
     "grade_id": "cell-81a9f70eee14c449",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test stand deviation is 1\n",
    "assert all(df.std() - 1 < 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Put things together\n",
    "\n",
    "Preparing a dataset for machine learning is a tedious process, which involves multiple steps in a specific order. Creating a pipeline or write a data clean module is one way to automate this process.\n",
    "\n",
    "Initially, I planned to ask you to implement a full pipeline using the Pipeline class in the `scikit-learn` library but found a few issues in the implementation. Therefore, I provide a reference solution to implement a basic data cleaning task and then invoke them in a sequence. Although you are not required to write the code in this assignment, you may still go through the code so that you know how to start when you implement a data cleaning process in your own project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. import required modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# 1. Get the data\n",
    "housing = pd.read_csv(\"input/housing.csv\")\n",
    "assert isinstance(housing, pd.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Separate numeric and categorical features\n",
    "num_attribs = [col for col in housing.columns if col not in [\"median_house_value\", \"ocean_proximity\"]]\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "ocean_proximity_categories =  list(housing[\"ocean_proximity\"].value_counts().index)\n",
    "attribute_caps = housing[['median_income', 'housing_median_age', 'median_house_value']].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Remove capped values\n",
    "def remove_capped_samples(df):\n",
    "    # Record attribute cap\n",
    "    indices = np.logical_and((df['median_house_value'] < attribute_caps['median_house_value']).values,\n",
    "                             (df['housing_median_age'] < attribute_caps['housing_median_age']).values,\n",
    "                             (df['median_income'] < attribute_caps['median_income']).values)\n",
    "    df = df[indices]\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = remove_capped_samples(housing)\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Impute missing values\n",
    "def impute_missing_values(df, numeric_attribs):\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "    _ = imputer.fit(df[numeric_attribs])\n",
    "    df_num = pd.DataFrame(imputer.transform(df[numeric_attribs]), columns=numeric_attribs)\n",
    "    return df_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = impute_missing_values(housing, num_attribs)\n",
    "df_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Adds combined attributes\n",
    "def add_combined_attributes(df):\n",
    "    df['rooms_per_household'] = df['total_rooms'] / df['households']\n",
    "    df['population_per_household'] = df['population'] / df['households']\n",
    "    df['bedrooms_per_room'] = df['total_bedrooms'] / df['total_rooms']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = add_combined_attributes(df_num)\n",
    "df_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Encode categorical data\n",
    "def encode_categorical_data(df, cat_attribs):\n",
    "    df_cat = None\n",
    "    for attr in cat_attribs:\n",
    "        if not df_cat:\n",
    "            df_cat = pd.get_dummies(df[attr], prefix=attr)\n",
    "        else:\n",
    "            df_cat = pd.concat(df_cat, pd.get_dummies(df[attr], prefix=attr))\n",
    "    return df_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = encode_categorical_data(housing, cat_attribs)\n",
    "df_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Combine column transfermations\n",
    "def transform_columns(df):\n",
    "    target = df['median_house_value']\n",
    "    \n",
    "    df_num = impute_missing_values(df, num_attribs)\n",
    "    add_combined_attributes(df_num)\n",
    "    \n",
    "    df_cat = encode_categorical_data(df, cat_attribs)\n",
    "      \n",
    "    df = pd.concat([df_num, df_cat, target], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "housing = transform_columns(housing)\n",
    "housing.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Split data into training data and testing data\n",
    "def stratified_split(df, test_size=0.25, random_state=0):\n",
    "    # Cut data by median_income\n",
    "    bins = [0., 1.5, 3.0, 4.5, 6.0, 8.0, 10.0, np.inf]\n",
    "    df['income_level'] = pd.cut(df['median_income'], bins, labels=range(1, len(bins)))\n",
    "\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    for train_index, test_index in split.split(df, df[\"income_level\"]):\n",
    "        train_data_ = df.iloc[train_index].copy()\n",
    "        test_data_ = df.iloc[test_index].copy()\n",
    "        break\n",
    "    for dataset_ in (train_data_, test_data_):\n",
    "        dataset_.drop(\"income_level\", axis=1, inplace=True)\n",
    "\n",
    "    # Reset the index to simplify later columns joins\n",
    "    train_data_ = train_data_.reset_index(drop=True)\n",
    "    test_data_ = test_data_.reset_index(drop=True)\n",
    "    return train_data_, test_data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train, housing_test = stratified_split(housing, 0.25, 6300)\n",
    "housing_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Save the cleaned data\n",
    "housing_train.to_csv(\"input/housing_train_cleaned.csv\", index=False)\n",
    "housing_test.to_csv(\"input/housing_test_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Create scaled data\n",
    "def scale_cleaned_data():\n",
    "    df_train = pd.read_csv(\"input/housing_train_cleaned.csv\")  \n",
    "\n",
    "    columns_to_scale = df_train.columns[0:11]\n",
    "    columns_no_sacle = df_train.columns[11:]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df_train[columns_to_scale])\n",
    "    \n",
    "    num_tr = scaler.transform(df_train[columns_to_scale])\n",
    "    df_num_tr = pd.DataFrame(num_tr, columns=columns_to_scale)\n",
    "    df_train = pd.concat([df_num_tr, df_train[columns_no_sacle]], axis=1)\n",
    "    df_train.to_csv(\"input/housing_train_scaled.csv\", index=False)\n",
    "    \n",
    "    df_test = pd.read_csv(\"input/housing_test_cleaned.csv\")\n",
    "    num_tr = scaler.transform(df_test[columns_to_scale])\n",
    "    df_num_tr = pd.DataFrame(num_tr, columns=columns_to_scale)\n",
    "    df_test = pd.concat([df_num_tr, df_test[columns_no_sacle]], axis=1)\n",
    "    df_test.to_csv(\"input/housing_test_scaled.csv\", index=False)\n",
    "    \n",
    "\n",
    "scale_cleaned_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "assert all([os.path.exists(\"input/housing_{}_cleaned.csv\".format(fn)) for fn in ['train', 'test']])\n",
    "assert all([os.path.exists(\"input/housing_{}_scaled.csv\".format(fn)) for fn in ['train', 'test']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_1 = pd.read_csv('input/housing_train_cleaned.csv')\n",
    "df_test_1 = pd.read_csv('input/housing_test_cleaned.csv')\n",
    "assert df_test_1.shape[1] == df_train_1.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if there are missing data\n",
    "assert all(df_test_1.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if a combined attributes exists\n",
    "assert 'rooms_per_household' in df_test_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if categorical features are transformed correctly\n",
    "assert all([sorted(list(df_test_1.iloc[::, pos].unique())) == list([0, 1]) for pos in range(11,16)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12b26ce76c743aae73c43c717a39d3e4",
     "grade": false,
     "grade_id": "cell-6824d92a0bb6b875",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "__Question 8(a)__. In the features scaling step, we didn't scale the traget variable. __Could you explain why?__ (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82a48fd4566e6bda29cab66d43626815",
     "grade": false,
     "grade_id": "cell-d65c17e54068f043",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1413d1d4fe46dce752cf6e8ede0fa409",
     "grade": false,
     "grade_id": "cell-695b2de407b1feda",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "__Question 8(b)__. In the above example, we use the feature means to replace the missing values in the data. This works for the numeric features. For categorical features, what would you do for the missing data? (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42be3cd015a4329b7c4496e67f1eed70",
     "grade": false,
     "grade_id": "cell-32ff77d7dd86cf33",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 8(c)__. In the feature scaling step, I use the cleaned train data to fit a standard scaler and then apply the scaler to scale both train data and test data. Could you explain why I should not fit a new standard scaler from the test data and use such scaler to scale the test data? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ac9cc95662b1b6a45631fabe5ffae14e",
     "grade": false,
     "grade_id": "cell-14a5d68441f149ac",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 8(d)__. Another subtle issue occurs when applying the OneHotEncode() to train data and test data separately but not all the categorical levels are included in both data sets. Can you explain why this is a problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ebe572207ab6bbd98a50cd38f3491e83",
     "grade": false,
     "grade_id": "cell-caacbe1249973140",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__End of Part C__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
